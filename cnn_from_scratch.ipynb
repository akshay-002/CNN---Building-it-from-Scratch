{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b23f6c-84ae-40e4-998f-c60d0c427e52",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a83d36; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h1 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 32px;\">Understanding CNNs from Scratch</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d564b35-4073-499d-80fe-1ee667f74ede",
   "metadata": {},
   "source": [
    "# Developing Custom Functions for CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T16:29:32.175785Z",
     "start_time": "2024-05-15T16:29:30.944462Z"
    }
   },
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b77fa8-46cb-458d-8716-7584fdfd878e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #642420; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h2 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 24px;\">Zero-Padding</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2382e6f-fc9a-45d2-8195-2389513f13bb",
   "metadata": {},
   "source": [
    "Zero-padding adds zeros around the border of an image:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca1e305-ca54-43c6-beb8-4fa60e73fbcc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images/padding.png\" alt=\"gg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00a7e8cb-b568-46aa-8211-41567f5f21bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n",
    "    \"\"\"\n",
    "    # Apply zero-padding to the input volume X.\n",
    "    X_pad=np.pad(X,((0,0), (pad,pad),(pad,pad),(0,0)))\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e29a1a-3f95-4b09-a61a-d8011f960660",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #642420; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h2 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 24px;\">Single Step of Convolution</h2>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0ca99-0e72-4d35-8f14-e57534c54873",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\">\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45dc51c6-4851-4b2d-879a-1231e0c7d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "    s = a_slice_prev * W\n",
    "    \n",
    "    # Sum over all entries of the volume s.\n",
    "    Z =np.sum(s)\n",
    "\n",
    "    # Ensure b is a float for correct addition.\n",
    "    b= np.float64(b)\n",
    "\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z= Z+b\n",
    "\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e012c84-f170-45b1-bbc7-0ece38919a69",
   "metadata": {},
   "source": [
    " \n",
    "<div style=\"background-color: #642420; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h2 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 24px;\">CNN - Forward Pass</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf773a6-f9d4-4af2-9685-3d51631c6ac7",
   "metadata": {},
   "source": [
    "In the forward pass, you will take many filters and convolve them on the input. Each 'convolution' gives you a 2D matrix output. You will then stack these outputs to get a 3D volume:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1806ddc-868d-4896-965e-ede5a39e9d94",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "The formulas relating the output shape of the convolution to the input shape are:\n",
    "    \n",
    "$$n_H = \\Bigl\\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_W = \\Bigl\\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\Bigr\\rfloor +1$$\n",
    "$$n_C = \\text{number of filters used in the convolution}$$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db12b7ed-7912-4984-9b3c-4b0bfe1b1f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, \n",
    "        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" \n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. \n",
    "    n_H = int((n_H_prev + 2*pad - f)/stride) + 1\n",
    "    n_W = int((n_W_prev + 2*pad - f)/stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros.\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m):               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]               # Select ith training example's padded activation\n",
    "        for h in range(n_H):           # loop over vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\"\n",
    "            vert_start = stride * h \n",
    "            vert_end = vert_start  + f\n",
    "            \n",
    "            for w in range(n_W):       # loop over horizontal axis of the output volume\n",
    "                # Find the horizontal start and end of the current \"slice\"\n",
    "                horiz_start = stride * w \n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(n_C):   # loop over channels (= #filters) of the output volume\n",
    "                                        \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell).\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron.\n",
    "                    weights = W[:, :, :, c]\n",
    "                    biases  = b[:, :, :, c]\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ef7c72-2d03-478d-b99e-16540634dd5a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style=\"background-color: #642420; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h2 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 24px;\">Pooling Layer</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d842fb-09b2-4f0c-a06c-a5a32013dd33",
   "metadata": {},
   "source": [
    "\n",
    "- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n",
    "\n",
    "- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43bb17-4c55-48ca-9ddf-bc88f0a5ff16",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1390d2c3-7637-48c1-9783-1fedf6a6b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        a_prev_slice = A_prev[i]\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" \n",
    "            vert_start = stride * h\n",
    "            vert_end = vert_start  + f\n",
    "            \n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" \n",
    "                horiz_start = stride * w\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. \n",
    "                    a_slice_prev = a_prev_slice[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. \n",
    "                    # Use an if statement to differentiate the modes. \n",
    "                    # Use np.max and np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_slice_prev)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_slice_prev)\n",
    "    \n",
    "                    else:\n",
    "                        print(mode+ \"-type pooling layer NOT Defined\")   \n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cfcd4c-f3e2-4bfc-b777-9db817d3ed38",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #642420; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h2 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 24px;\"> Backpropagation in CNN</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca80847-767f-4d35-b264-a8452f15602a",
   "metadata": {},
   "source": [
    "### Convolutional Layer - Backward Pass \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce412ba-4068-48f4-b939-510480667b1b",
   "metadata": {},
   "source": [
    "#### Computing dA:\n",
    "This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:\n",
    "\n",
    "$$dA \\mathrel{+}= \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "#### Computing dW:\n",
    "This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:\n",
    "\n",
    "$$dW_c  \\mathrel{+}= \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "#### Computing db:\n",
    "\n",
    "This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n",
    "\n",
    "$$db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1606efe1-5b4e-4b81-9ba6-dce5b2f7b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"    \n",
    "    \n",
    "        \n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros(A_prev.shape)            \n",
    "    dW = np.zeros(W.shape)       \n",
    "    db = np.zeros(b.shape)       \n",
    "    \n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = stride * h \n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = stride * w\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpadded da_prev_pad\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e512cd31-ac9d-4abc-be83-98d339d8e0ef",
   "metadata": {},
   "source": [
    "### Max Pooling - Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f441d0-8479-45b0-b330-ad36026dd81d",
   "metadata": {},
   "source": [
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd0b17f8-854d-476c-a7fc-e0c8a5ab0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"    \n",
    "    \n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4befea5e-7b89-485d-bd04-4fe00636e417",
   "metadata": {},
   "source": [
    "### Average Pooling - Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522833b-1bcf-4b94-97c6-c03119c405de",
   "metadata": {},
   "source": [
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4804ff1-ee20-47db-a23b-a7c8e1f0ffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"    \n",
    "    # Retrieve dimensions from shape \n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix \n",
    "    average = np.prod(shape)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value \n",
    "    a = (dz/average)*np.ones(shape)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc728bf8-c322-4963-beea-d2d788cf23c5",
   "metadata": {},
   "source": [
    "### Putting it Together: Pool Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0e4e872-3f1a-4aed-aa2d-dd995b6db52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    # Retrieve information from cache\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m): # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev\n",
    "        a_prev = A_prev[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "        \n",
    "                    # Find the corners of the current \"slice\" \n",
    "                    vert_start  = h * stride\n",
    "                    vert_end    = h * stride + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end   = w * stride + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev\n",
    "                        a_prev_slice = a_prev[ vert_start:vert_end, horiz_start:horiz_end, c ]\n",
    "                        \n",
    "                        # Create the mask from a_prev_slice\n",
    "                        mask = create_mask_from_window( a_prev_slice )\n",
    "\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value da from dA\n",
    "                        da = dA[i, h, w, c]\n",
    "                        \n",
    "                        # Define the shape of the filter as fxf\n",
    "                        shape = (f,f)\n",
    "\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da.\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2557646-8bf2-431f-9812-80f03b9496cb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #642420; color: #FFFFFF; padding: 5px; text-align: center; border-radius: 15px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.5);\">\n",
    "  <h2 style=\"margin: 0; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: bold; font-size: 24px;\"> Example Usage</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c862cb-360d-45d6-bfdb-498530d277b2",
   "metadata": {},
   "source": [
    "## Implementing zero_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "56aaf401-ad0f-4a97-b4c2-ff1b8d06b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      " (4, 3, 3, 2)\n",
      "x_pad.shape =\n",
      " (4, 9, 9, 2)\n",
      "x[1,1] =\n",
      " [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] =\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbd7dfebd30>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## zero-padding function to pad a 4D input tensor and visualize the padding.\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 3)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1, 1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1, 1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0, :, :, 0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0, :, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadd85a-d2bb-4717-809b-f1411035521d",
   "metadata": {},
   "source": [
    "## conv_single_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9f6c3548-dc76-4b9b-acd6-dcb9e6b6df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "# convolutional single step function to compute the output of a single convolutional neuron.\n",
    "\n",
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)\n",
    "\n",
    "assert (type(Z) == np.float64)\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1a5712-3b3f-4d50-bb27-69cf0c21cf3d",
   "metadata": {},
   "source": [
    "## conv_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91644b2e-4af6-4463-8b00-74008b4d1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.5511276474566768\n",
      "Z[0,2,1] =\n",
      " [-2.17796037  8.07171329 -0.5772704   3.36286738  4.48113645 -2.89198428\n",
      " 10.99288867  3.03171932]\n",
      "cache_conv[0][1][2][3] =\n",
      " [-1.1191154   1.9560789  -0.3264995  -1.34267579]\n"
     ]
    }
   ],
   "source": [
    "# convolutional forward function to perform forward propagation through a convolutional layer.\n",
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 7, 4)\n",
    "W = np.random.randn(3, 3, 4, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 1,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "z_mean = np.mean(Z)\n",
    "z_0_2_1 = Z[0, 2, 1]\n",
    "cache_0_1_2_3 = cache_conv[0][1][2][3]\n",
    "print(\"Z's mean =\\n\", z_mean)\n",
    "print(\"Z[0,2,1] =\\n\", z_0_2_1)\n",
    "print(\"cache_conv[0][1][2][3] =\\n\", cache_0_1_2_3)\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13747229-4072-411b-b9df-da59ac200c13",
   "metadata": {},
   "source": [
    "## Pool_Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7582590-d5a7-4afb-b1fa-6af04a672aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "\n",
      "\n",
      "mode = max\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[1.96710175 0.84616065 1.27375593]\n",
      " [1.96710175 0.84616065 1.23616403]\n",
      " [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 3, 3, 3)\n",
      "A[1, 1] =\n",
      " [[ 0.44497696 -0.00261695 -0.31040307]\n",
      " [ 0.50811474 -0.23493734 -0.23961183]\n",
      " [ 0.11872677  0.17255229 -0.22112197]]\n",
      "\n",
      "\n",
      "\u001b[0mCASE 2:\n",
      "\n",
      "mode = max\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[0] =\n",
      " [[[1.74481176 0.90159072 1.65980218]\n",
      "  [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      " [[1.13162939 1.51981682 2.18557541]\n",
      "  [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "mode = average\n",
      "A.shape = (2, 2, 2, 3)\n",
      "A[1] =\n",
      " [[[-0.17313416  0.32377198 -0.34317572]\n",
      "  [ 0.02030094  0.14141479 -0.01231585]]\n",
      "\n",
      " [[ 0.42944926  0.08446996 -0.27290905]\n",
      "  [ 0.15077452  0.28911175  0.00123239]]]\n",
      "\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# pooling forward function to perform max and average pooling with different stride values.\n",
    "\n",
    "## Case 1: stride of 1\n",
    "print(\"CASE 1:\\n\")\n",
    "print()\n",
    "np.random.seed(1)\n",
    "A_prev_case_1 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_1 = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_1, hparameters_case_1, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1, 1] =\\n\", A[1, 1])\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "## Case 2: stride of 2\n",
    "print(\"\\n\\033[0mCASE 2:\\n\")\n",
    "np.random.seed(1)\n",
    "A_prev_case_2 = np.random.randn(2, 5, 5, 3)\n",
    "hparameters_case_2 = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[0] =\\n\", A[0])\n",
    "print()\n",
    "\n",
    "A, cache = pool_forward(A_prev_case_2, hparameters_case_2, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A[1] =\\n\", A[1])\n",
    "print()\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75729ddc-d0a6-4457-a580-f616833f1383",
   "metadata": {},
   "source": [
    "## Conv_Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81ceb4b3-5c4f-4a11-b385-84acac299fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# Convolutional backward function to compute gradients for convolutional layer parameters.\n",
    "\n",
    "## We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "## which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10, 4, 4, 3)\n",
    "W = np.random.randn(2, 2, 3, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))\n",
    "\n",
    "assert type(dA) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert type(dW) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert type(db) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert dA.shape == (10, 4, 4, 3), f\"Wrong shape for dA  {dA.shape} != (10, 4, 4, 3)\"\n",
    "assert dW.shape == (2, 2, 3, 8), f\"Wrong shape for dW {dW.shape} != (2, 2, 3, 8)\"\n",
    "assert db.shape == (1, 1, 1, 8), f\"Wrong shape for db {db.shape} != (1, 1, 1, 8)\"\n",
    "assert np.isclose(np.mean(dA), 1.4524377), \"Wrong values for dA\"\n",
    "assert np.isclose(np.mean(dW), 1.7269914), \"Wrong values for dW\"\n",
    "assert np.isclose(np.mean(db), 7.8392325), \"Wrong values for db\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b11fc-fc7f-43c1-8393-22d92f91c17b",
   "metadata": {},
   "source": [
    "## Max Pooling - Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e8be0434-c7f4-4188-bbbd-b0f88ddd5cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# create_mask_from_window function to create a mask based on the maximum value in a window.\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(2, 3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)\n",
    "\n",
    "x = np.array([[-1, 2, 3],\n",
    "              [2, -3, 2],\n",
    "              [1, 5, -2]])\n",
    "\n",
    "y = np.array([[False, False, False],\n",
    "     [False, False, False],\n",
    "     [False, True, False]])\n",
    "mask = create_mask_from_window(x)\n",
    "\n",
    "assert type(mask) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert mask.shape == x.shape, \"Input and output shapes must match\"\n",
    "assert np.allclose(mask, y), \"Wrong output. The True value must be at position (2, 1)\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1ad42-3260-441a-9883-8d2759bc4eec",
   "metadata": {},
   "source": [
    "## Average Pooling - Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "936b0b0c-72f1-4c5c-889f-41b1391eab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# distribute_value function to distribute a value evenly across a given shape.\n",
    "\n",
    "a = distribute_value(2, (2, 2))\n",
    "print('distributed value =', a)\n",
    "\n",
    "\n",
    "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert a.shape == (2, 2), f\"Wrong shape {a.shape} != (2, 2)\"\n",
    "assert np.sum(a) == 2, \"Values must sum to 2\"\n",
    "\n",
    "a = distribute_value(100, (10, 10))\n",
    "assert type(a) == np.ndarray, \"Output must be a np.ndarray\"\n",
    "assert a.shape == (10, 10), f\"Wrong shape {a.shape} != (10, 10)\"\n",
    "assert np.sum(a) == 100, \"Values must sum to 100\"\n",
    "\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb700c3-929f-44ec-8792-3c1111e9de02",
   "metadata": {},
   "source": [
    "## Pool_Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f810a59c-a6df-4f10-a66a-5bb197149e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4, 2, 2)\n",
      "(5, 5, 3, 2)\n",
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev1[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev2[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n",
      "\u001b[92m All tests passed.\n"
     ]
    }
   ],
   "source": [
    "# pool_backward function to compute gradients for the pooling layer in both max and average pooling modes.\n",
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(A.shape)\n",
    "print(cache[0].shape)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev1 = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev1[1,1] = ', dA_prev1[1, 1])  \n",
    "print()\n",
    "dA_prev2 = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev2[1,1] = ', dA_prev2[1, 1]) \n",
    "\n",
    "assert type(dA_prev1) == np.ndarray, \"Wrong type\"\n",
    "assert dA_prev1.shape == (5, 5, 3, 2), f\"Wrong shape {dA_prev1.shape} != (5, 5, 3, 2)\"\n",
    "assert np.allclose(dA_prev1[1, 1], [[0, 0], \n",
    "                                    [ 5.05844394, -1.68282702],\n",
    "                                    [ 0, 0]]), \"Wrong values for mode max\"\n",
    "assert np.allclose(dA_prev2[1, 1], [[0.08485462,  0.2787552], \n",
    "                                    [1.26461098, -0.25749373], \n",
    "                                    [1.17975636, -0.53624893]]), \"Wrong values for mode average\"\n",
    "print(\"\\033[92m All tests passed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
